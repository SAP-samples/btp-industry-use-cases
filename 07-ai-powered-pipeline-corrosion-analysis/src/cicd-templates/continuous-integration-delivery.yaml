{
    "apiVersion": "argoproj.io/v1alpha1",
    "kind": "WorkflowTemplate",
    "metadata": {
        "name": "full-cicd",
        "labels": {
            "metaflow/metaflow_version": "2.8.0",
            "metaflow/python_version": "3.10.4",
            "metaflow/user": "i559573",
            "metaflow/runtime": "dev",
            "scenarios.ai.sap.com/id": "cicd-pipeline-corrosion-analytics",
            "ai.sap.com/version": "0.1.0",
            "app": "metaflow",
            "metaflow/workflow_template": "trainflow",
            "app.kubernetes.io/created-by": "I559573"
        },
        "annotations": {
            "scenarios.ai.sap.com/name": "cicd-pipeline-corrosion-analytics",
            "executables.ai.sap.com/name": "full-cicd",
            "metaflow/flow_name": "TrainFlow"
        }
    },
    "spec": {
        "entrypoint": "entry",
        "workflowMetadata": {
            "labels": {
                "app": "metaflow",
                "metaflow/workflow_template": "trainflow",
                "app.kubernetes.io/created-by": "I559573"
            },
            "annotations": {
                "metaflow/flow_name": "TrainFlow"
            }
        },
        "imagePullSecrets": [
            {
                "name": "docker-registry-secret"
            }
        ],
        "parallelism": 100,
        "templates": [
            {
                "name": "entry",
                "dag": {
                    "tasks": [
                        {
                            "name": "start",
                            "template": "start",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "input-paths",
                                        "value": "{{workflow.name}}/_parameters/0"
                                    }
                                ]
                            }
                        },
                        {
                            "name": "train-model",
                            "template": "train-model",
                            "dependencies": [
                                "start"
                            ],
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "input-paths",
                                        "value": "{{workflow.name}}/start/{{tasks.start.outputs.parameters.task-id}}"
                                    }
                                ]
                            }
                        },
                        {
                            "name": "end",
                            "template": "end",
                            "dependencies": [
                                "train-model"
                            ],
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "input-paths",
                                        "value": "{{workflow.name}}/train_model/{{tasks.train-model.outputs.parameters.task-id}}"
                                    }
                                ]
                            }
                        }
                    ]
                }
            },
            {
                "name": "start",
                "metadata": {
                    "labels": {
                        "metaflow/metaflow_version": "2.8.0",
                        "metaflow/python_version": "3.10.4",
                        "metaflow/user": "i559573",
                        "metaflow/runtime": "dev",
                        "app": "metaflow",
                        "metaflow/workflow_template": "trainflow",
                        "app.kubernetes.io/created-by": "I559573",
                        "metaflow/step_name": "start",
                        "app.kubernetes.io/name": "metaflow-task",
                        "app.kubernetes.io/part-of": "metaflow"
                    },
                    "annotations": {
                        "metaflow/flow_name": "TrainFlow",
                        "metaflow/attempt": "0"
                    }
                },
                "activeDeadlineSeconds": 432000,
                "inputs": {
                    "parameters": [
                        {
                            "name": "input-paths"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "task-id",
                            "value": "{{pod.name}}"
                        }
                    ]
                },
                "container": {
                    "image": "docker.io/yoshidj/metaflow-xyz:test1",
                    "command": [
                        "bash"
                    ],
                    "args": [
                        "-c",
                        "true && export PYTHONUNBUFFERED=x MF_PATHSPEC=TrainFlow/{{workflow.name}}/start/{{pod.name}} MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=/tmp/mflog_stdout MFLOG_STDERR=/tmp/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog 'Setting up task environment.' && python -m pip install requests -qqq && python -m pip install awscli boto3 -qqq && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog 'Downloading code package...'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f job.tar >/dev/null && mflog 'Code package downloaded.' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog 'Failed to download code package from s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f after 6 tries. Exiting...' && exit 1; fi && TAR_OPTIONS='--warning=no-timestamp' tar xf job.tar && mflog 'Task is starting.' && (python trainflow.py --quiet --metadata=local --environment=local --datastore=s3 --event-logger=nullSidecarLogger --monitor=nullSidecarMonitor --no-pylint init --run-id {{workflow.name}} --task-id {{pod.name}}-params && python trainflow.py --quiet --metadata=local --environment=local --datastore=s3 --datastore-root=s3://ai-sustainability-dataset/Metaflow --event-logger=nullSidecarLogger --monitor=nullSidecarMonitor --no-pylint --with=argo_internal step start --run-id {{workflow.name}} --task-id {{pod.name}} --retry-count 0 --max-user-code-retries 0 --input-paths {{workflow.name}}/_parameters/{{pod.name}}-params) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c"
                    ],
                    "env": [
                        {
                            "name": "METAFLOW_USER",
                            "value": "I559573"
                        },
                        {
                            "name": "METAFLOW_DATASTORE_SYSROOT_S3",
                            "value": "s3://ai-sustainability-dataset/Metaflow"
                        },
                        {
                            "name": "METAFLOW_DEFAULT_METADATA",
                            "value": "local"
                        }
                    ],
                    "envFrom": [
                        {
                            "secretRef": {
                                "name": "default-object-store-secret"
                            }
                        }
                    ],
                    "resources": {
                        "requests": {
                            "ephemeral-storage": "10240Mi"
                        },
                        "limits": {
                            "ephemeral-storage": "10240Mi"
                        }
                    }
                }
            },
            {
                "name": "train-model",
                "metadata": {
                    "labels": {
                        "metaflow/metaflow_version": "2.8.0",
                        "metaflow/python_version": "3.10.4",
                        "metaflow/user": "i559573",
                        "metaflow/runtime": "dev",
                        "app": "metaflow",
                        "metaflow/workflow_template": "trainflow",
                        "app.kubernetes.io/created-by": "I559573",
                        "metaflow/step_name": "train-model",
                        "app.kubernetes.io/name": "metaflow-task",
                        "app.kubernetes.io/part-of": "metaflow"
                    },
                    "annotations": {
                        "metaflow/flow_name": "TrainFlow",
                        "metaflow/attempt": "0"
                    }
                },
                "activeDeadlineSeconds": 432000,
                "inputs": {
                    "parameters": [
                        {
                            "name": "input-paths"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "task-id",
                            "value": "{{pod.name}}"
                        }
                    ]
                },
                "container": {
                    "image": "docker.io/yoshidj/metaflow-xyz:test1",
                    "command": [
                        "bash"
                    ],
                    "args": [
                        "-c",
                        "true && export PYTHONUNBUFFERED=x MF_PATHSPEC=TrainFlow/{{workflow.name}}/train_model/{{pod.name}} MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=/tmp/mflog_stdout MFLOG_STDERR=/tmp/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog 'Setting up task environment.' && python -m pip install requests -qqq && python -m pip install awscli boto3 -qqq && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog 'Downloading code package...'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f job.tar >/dev/null && mflog 'Code package downloaded.' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog 'Failed to download code package from s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f after 6 tries. Exiting...' && exit 1; fi && TAR_OPTIONS='--warning=no-timestamp' tar xf job.tar && mflog 'Task is starting.' && (python trainflow.py --quiet --metadata=local --environment=local --datastore=s3 --datastore-root=s3://ai-sustainability-dataset/Metaflow --event-logger=nullSidecarLogger --monitor=nullSidecarMonitor --no-pylint --with=argo_internal step train_model --run-id {{workflow.name}} --task-id {{pod.name}} --retry-count 0 --max-user-code-retries 0 --input-paths {{inputs.parameters.input-paths}}) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c"
                    ],
                    "env": [
                        {
                            "name": "METAFLOW_USER",
                            "value": "I559573"
                        },
                        {
                            "name": "METAFLOW_DATASTORE_SYSROOT_S3",
                            "value": "s3://ai-sustainability-dataset/Metaflow"
                        },
                        {
                            "name": "METAFLOW_DEFAULT_METADATA",
                            "value": "local"
                        }
                    ],
                    "envFrom": [
                        {
                            "secretRef": {
                                "name": "default-object-store-secret"
                            }
                        }
                    ],
                    "resources": {
                        "requests": {
                            "ephemeral-storage": "10240Mi"
                        },
                        "limits": {
                            "ephemeral-storage": "10240Mi"
                        }
                    }
                }
            },
            {
                "name": "end",
                "metadata": {
                    "labels": {
                        "metaflow/metaflow_version": "2.8.0",
                        "metaflow/python_version": "3.10.4",
                        "metaflow/user": "i559573",
                        "metaflow/runtime": "dev",
                        "app": "metaflow",
                        "metaflow/workflow_template": "trainflow",
                        "app.kubernetes.io/created-by": "I559573",
                        "metaflow/step_name": "end",
                        "app.kubernetes.io/name": "metaflow-task",
                        "app.kubernetes.io/part-of": "metaflow"
                    },
                    "annotations": {
                        "metaflow/flow_name": "TrainFlow",
                        "metaflow/attempt": "0"
                    }
                },
                "activeDeadlineSeconds": 432000,
                "inputs": {
                    "parameters": [
                        {
                            "name": "input-paths"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "task-id",
                            "value": "{{pod.name}}"
                        }
                    ]
                },
                "container": {
                    "image": "docker.io/yoshidj/metaflow-xyz:test1",
                    "command": [
                        "bash"
                    ],
                    "args": [
                        "-c",
                        "true && export PYTHONUNBUFFERED=x MF_PATHSPEC=TrainFlow/{{workflow.name}}/end/{{pod.name}} MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=/tmp/mflog_stdout MFLOG_STDERR=/tmp/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog 'Setting up task environment.' && python -m pip install requests -qqq && python -m pip install awscli boto3 -qqq && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog 'Downloading code package...'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f job.tar >/dev/null && mflog 'Code package downloaded.' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog 'Failed to download code package from s3://ai-sustainability-dataset/Metaflow/TrainFlow/data/2e/2eb7a7bd5a09207dd5a56b1ec4367bfd46cf7a7f after 6 tries. Exiting...' && exit 1; fi && TAR_OPTIONS='--warning=no-timestamp' tar xf job.tar && mflog 'Task is starting.' && (python trainflow.py --quiet --metadata=local --environment=local --datastore=s3 --datastore-root=s3://ai-sustainability-dataset/Metaflow --event-logger=nullSidecarLogger --monitor=nullSidecarMonitor --no-pylint --with=argo_internal step end --run-id {{workflow.name}} --task-id {{pod.name}} --retry-count 0 --max-user-code-retries 0 --input-paths {{inputs.parameters.input-paths}}) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c"
                    ],
                    "env": [
                        {
                            "name": "METAFLOW_USER",
                            "value": "I559573"
                        },
                        {
                            "name": "METAFLOW_DATASTORE_SYSROOT_S3",
                            "value": "s3://ai-sustainability-dataset/Metaflow"
                        },
                        {
                            "name": "METAFLOW_DEFAULT_METADATA",
                            "value": "local"
                        }
                    ],
                    "envFrom": [
                        {
                            "secretRef": {
                                "name": "default-object-store-secret"
                            }
                        }
                    ],
                    "resources": {
                        "requests": {
                            "ephemeral-storage": "10240Mi"
                        },
                        "limits": {
                            "ephemeral-storage": "10240Mi"
                        }
                    }
                }
            }
        ]
    }
}